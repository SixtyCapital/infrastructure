apiVersion: v1
kind: Service
metadata:
  name: airflow-web
spec:
  type: NodePort
  selector:
    app: airflow-web
    tier: web
  ports:
    - name: airflow-web
      protocol: TCP
      port: 8080
      targetPort: airflow-web
      nodePort: 32080
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: airflow-web
    tier: web
  name: airflow-web
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-web
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: airflow-web
    spec:
      containers:
      - name: airflow-web
        image: gcr.io/sixty-capital/investment:develop
        command:
        - /usr/local/bin/airflow-entry.sh
        - webserver
        env:
        - name: GOOGLE_CLOUD_PROJECT
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: project-id
        - name: SIXTY_CONFIG_FILE
          value: config_airflow.yaml
        - name: AIRFLOW__CORE__LOGGING_CONFIG_CLASS
          value: sixty.production.airflow_config.logging_config.LOGGING_CONFIG
        - name: AIRFLOW__CORE__REMOTE_LOG_CONN_ID
          value: sixty_gcp
        - name: AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: airflow-log-folder
        - name: AIRFLOW__CORE__TASK_LOG_READER
          value: gcs.task
        - name: FERNET_KEY
          value: Yd02MB6W0KXvklEG1FPxOJt_A-V3ARvnvp1TMfdPivc=
        - name: REDIS_HOST
          value: celery-redis
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-airflow-database
        - name: POSTGRES_HOST
          value: localhost
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-password
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            memory: 1Gi
          requests:
            memory: 1Gi
      - image: gcr.io/cloudsql-docker/gce-proxy:1.11
        name: cloudsql-proxy
        command: ["/cloud_sql_proxy"]
        args: ["-instances=$(DB_CONNECTION_NAME)=tcp:5432"]
        env:
        - name: DB_CONNECTION_NAME
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-connection-name
      restartPolicy: Always
      nodeSelector:
        cloud.google.com/gke-nodepool: small
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: airflow-scheduler
    tier: scheduler
  name: airflow-scheduler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow-scheduler
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: airflow-scheduler
    spec:
      containers:
      - name: airflow-scheduler
        image: gcr.io/sixty-capital/investment:develop
        command:
        - /usr/local/bin/airflow-entry.sh
        - scheduler
        # suggested to let this restart after X runs for reliability
        # https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb
        - --num_runs=10
        env:
        - name: GOOGLE_CLOUD_PROJECT
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: project-id
        - name: SIXTY_CONFIG_FILE
          value: config_airflow.yaml
        - name: AIRFLOW__CORE__LOGGING_CONFIG_CLASS
          value: sixty.production.airflow_config.logging_config.LOGGING_CONFIG
        - name: AIRFLOW__CORE__REMOTE_LOG_CONN_ID
          value: sixty_gcp
        - name: AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: airflow-log-folder
        - name: AIRFLOW__CORE__TASK_LOG_READER
          value: gcs.task
        - name: FERNET_KEY
          value: Yd02MB6W0KXvklEG1FPxOJt_A-V3ARvnvp1TMfdPivc=
        - name: REDIS_HOST
          value: celery-redis
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-airflow-database
        - name: POSTGRES_HOST
          value: localhost
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-password
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            memory: 2Gi
          requests:
            memory: 2Gi
      - image: gcr.io/cloudsql-docker/gce-proxy:1.11
        name: cloudsql-proxy
        command: ["/cloud_sql_proxy"]
        args: ["-instances=$(DB_CONNECTION_NAME)=tcp:5432"]
        env:
        - name: DB_CONNECTION_NAME
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-connection-name
      restartPolicy: Always
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: airflow-worker
    tier: worker
  name: airflow-worker
spec:
  replicas: 32
  selector:
    matchLabels:
      app: airflow-worker
  # strategy:
  #   rollingUpdate:
  #     maxSurge: 2
  #     maxUnavailable: 2
  #   type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: airflow-worker
    spec:
      containers:
      - name: airflow-worker
        image: gcr.io/sixty-capital/investment:develop
        command:
        - /usr/local/bin/airflow-entry.sh
        - worker
        - --concurrency=1
        env:
        - name: GOOGLE_CLOUD_PROJECT
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: project-id
        - name: SIXTY_CONFIG_FILE
          value: config_airflow.yaml
        - name: AIRFLOW__CORE__LOGGING_CONFIG_CLASS
          value: sixty.production.airflow_config.logging_config.LOGGING_CONFIG
        - name: AIRFLOW__CORE__REMOTE_LOG_CONN_ID
          value: sixty_gcp
        - name: AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: airflow-log-folder
        - name: AIRFLOW__CORE__TASK_LOG_READER
          value: gcs.task
        - name: FERNET_KEY
          value: Yd02MB6W0KXvklEG1FPxOJt_A-V3ARvnvp1TMfdPivc=
        - name: REDIS_HOST
          value: celery-redis
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-airflow-database
        - name: POSTGRES_HOST
          value: localhost
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-password
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          limits:
            memory: 4Gi
            cpu: 1000m
          requests:
            memory: 1Gi
            cpu: 300m
      - image: gcr.io/cloudsql-docker/gce-proxy:1.11
        name: cloudsql-proxy
        command: ["/cloud_sql_proxy"]
        args: ["-instances=$(DB_CONNECTION_NAME)=tcp:5432"]
        env:
        - name: DB_CONNECTION_NAME
          valueFrom:
            secretKeyRef:
              name: airflow-secrets
              key: db-connection-name
      restartPolicy: Always
      nodeSelector:
        cloud.google.com/gke-nodepool: large-preemptible
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: celery-redis
  # these labels can be applied automatically
  # from the labels in the pod template if not set
  # labels:
  #   app: redis
  #   role: master
  #   tier: backend
spec:
  # this replicas value is default
  # modify it according to your case
  replicas: 1
  # selector can be applied automatically
  # from the labels in the pod template if not set
  # selector:
  #   matchLabels:
  #     app: guestbook
  #     role: master
  #     tier: backend
  template:
    metadata:
      labels:
        app: celery-redis
        role: master
        tier: backend
    spec:
      containers:
      - name: celery-redis
        image: launcher.gcr.io/google/redis4
        args: [
          "--maxmemory", "2gb",
          "--maxmemory-policy", "allkeys-lru",
          "--timeout", "10800",  # 3 hours
          "--tcp-backlog", "10000"]
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: 2Gi
      volumes:
      - name: celery-redis-data
        persistentVolumeClaim:
          claimName: celery-redis-data
      nodeSelector:
        cloud.google.com/gke-nodepool: small
---
# Request a persistent volume from the cluster using a Persistent Volume Claim.
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: celery-redis-data
  annotations:
    volume.alpha.kubernetes.io/storage-class: default
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: celery-redis
  labels:
    app: celery-redis
    role: master
    tier: backend
spec:
  ports:
    # the port that this service should serve on
  - port: 6379
    targetPort: 6379
  selector:
    app: celery-redis
    role: master
    tier: backend
